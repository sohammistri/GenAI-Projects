{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG part 2\n",
    "\n",
    "## Query transformation\n",
    "\n",
    "The idea is that we can transform the query into something which can make retrieval easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 1: Multi query\n",
    "\n",
    "A single query be reframed into multiple queries, so that we can query into a vector DB multiple times and take the union of the documents fetched everytime. \n",
    "![rag-multi-query](rag_part_2_multi_query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do LLM agents break down complex tasks into simpler components?',\n",
       " 'What does task decomposition entail in the context of large language models?',\n",
       " 'Can you explain the process of task decomposition used by large language model agents?',\n",
       " 'What are the methods of task decomposition applied in large language models?',\n",
       " 'How do large language models manage task decomposition to handle complex problems?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "generate_queries.invoke({\"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down a complex task into smaller, more manageable subtasks. This technique is crucial for enhancing the performance of LLMs on intricate tasks that require multiple steps or stages. The decomposition not only simplifies the problem but also provides a structured approach that the LLM can follow, making it easier to generate coherent and contextually appropriate responses.\n",
       "\n",
       "In the context of LLM-powered autonomous agents, task decomposition can be implemented in several ways:\n",
       "\n",
       "1. **Chain of Thought (CoT)**: This is a prompting technique where the model is instructed to \"think step by step.\" This method involves the LLM using more test-time computation to logically break down a task into simpler steps. The CoT approach helps the model to process and articulate each step, thereby improving its ability to handle complex tasks by focusing on one component at a time.\n",
       "\n",
       "2. **Tree of Thoughts**: An extension of the CoT, this technique explores multiple reasoning possibilities at each decomposition step. It creates a tree structure where each node represents a thought or a sub-task, and branches represent different approaches or solutions to the sub-task. This method can utilize search techniques like breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or a majority vote to determine the most promising path forward.\n",
       "\n",
       "3. **Task-Specific Instructions**: LLMs can also be guided through task decomposition by using specific instructions tailored to the task at hand. For example, if the task is to write a novel, the instruction might be \"Write a story outline,\" which prompts the LLM to first outline the plot before delving into detailed writing.\n",
       "\n",
       "4. **Human Inputs**: Involving human inputs for task decomposition allows for a hybrid approach where humans and LLMs collaborate. Humans can provide initial guidance on breaking down the task, and the LLM can then proceed with detailed execution based on these guidelines.\n",
       "\n",
       "Overall, task decomposition is a fundamental strategy in the design of LLM-powered autonomous agents, enabling them to tackle complex tasks more effectively by reducing cognitive load and providing clear execution paths."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_rag_chain.invoke({\"question\":question})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided documents, here are some key sections of the article described in a pointwise manner:\n",
       "\n",
       "1. **ReAct Prompt Template**:\n",
       "   - Introduces a structured format for Language Model (LLM) operations, which includes steps like Thought, Action, and Observation, repeated multiple times to guide the LLM's processing and response generation.\n",
       "\n",
       "2. **Challenges in Building LLM-Centered Agents**:\n",
       "   - Discusses common limitations encountered when developing agents centered around large language models, highlighting practical challenges in implementation and operation.\n",
       "\n",
       "3. **Detailed Coding Instructions**:\n",
       "   - Provides comprehensive guidelines for coding, including the layout of core classes, functions, and methods with comments on their purposes.\n",
       "   - Specifies the format for outputting code in markdown blocks, ensuring that filenames and languages are appropriately formatted and that the code is fully functional and compatible across different files.\n",
       "\n",
       "4. **Performance Evaluation**:\n",
       "   - Outlines methods for continuous review and analysis of actions to ensure optimal performance.\n",
       "   - Emphasizes the importance of self-criticism, reflection on past decisions, and efficient command execution to minimize resource usage.\n",
       "\n",
       "5. **Resources for Agent Operation**:\n",
       "   - Lists resources available to the agent, such as internet access, long-term memory management, and GPT-3.5 powered agents for task delegation.\n",
       "\n",
       "6. **Experiments and Observations**:\n",
       "   - Mentions specific experiments like those conducted in AlfWorld Env and HotpotQA, noting particular issues like hallucination and inefficient planning.\n",
       "\n",
       "These sections collectively provide insights into the structure, challenges, and operational guidelines for building and managing LLM-centered agents, as well as evaluating their performance in practical scenarios."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_rag_chain.invoke({\"question\":\"Describe some key sections of the article in a pointwise manner.\"})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The article mentions a study titled \"Generative Agents\" by Park et al. (2023), which is a significant experiment in the area of Generative Agents Simulation. In this study, 25 virtual characters, each controlled by a large language model (LLM)-powered agent, interact within a sandbox environment inspired by The Sims. This simulation aims to create believable simulacra of human behavior for interactive applications.\n",
       "\n",
       "The design of these generative agents integrates LLMs with memory, planning, and reflection mechanisms. This allows the agents to behave based on past experiences and interact with other agents. A key feature of this system is the memory stream, which is an external database functioning as a long-term memory module. It records a comprehensive list of the agents' experiences in natural language, capturing each observation or event directly provided by the agent. This memory stream enables the agents to recall past interactions and use this information to inform future behaviors and decisions.\n",
       "\n",
       "Inter-agent communication within this simulation can trigger new natural language statements, enhancing the dynamic and interactive nature of the environment. This study showcases the potential of LLMs to simulate complex social interactions and behaviors in a controlled virtual setting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_rag_chain.invoke({\"question\":\"Describe some of the studies mentioned in this article done in the area of 'Generative Agents Simulation' . \"})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MultiQueryRetriever\n",
    "\n",
    "Source: [Link](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What methods are used for breaking down tasks in large language models?', '', 'How do large language models handle task decomposition?', '', 'Can you describe the strategies for task decomposition in large language models?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition in context of LLMs?\"\n",
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retriever_from_llm, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What methods are used for breaking down tasks in large language models?', '', 'How do large language models handle task decomposition?', '', 'Can you describe the strategies for task decomposition in large language models?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The approaches to task decomposition in the context of Large Language Models (LLMs) as described in the provided documents include:\n",
       "\n",
       "1. **Chain of Thought (CoT)**: Introduced by Wei et al. in 2022, this technique involves instructing the model to \"think step by step.\" This method allows the LLM to use more test-time computation to break down complex tasks into smaller, more manageable steps. The CoT approach helps in transforming large tasks into multiple manageable tasks and provides insights into the modelâ€™s reasoning process.\n",
       "\n",
       "2. **Tree of Thoughts**: An extension of the CoT approach, developed by Yao et al. in 2023, which explores multiple reasoning possibilities at each step. It starts by decomposing the problem into multiple thought steps and then generates multiple thoughts per step, creating a tree structure. The search process within this structure can be conducted using either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier (via a prompt) or by majority vote.\n",
       "\n",
       "3. **Simple Prompting Techniques**: These involve using straightforward prompts to guide the LLM in decomposing tasks. Examples include prompts like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", or task-specific instructions such as \"Write a story outline.\" for writing a novel.\n",
       "\n",
       "4. **Human Inputs**: Involving human interaction to guide or adjust the task decomposition process, ensuring that the decomposition aligns with human understanding and requirements.\n",
       "\n",
       "These approaches highlight the versatility and adaptability of LLMs in handling complex tasks by breaking them down into simpler, more digestible components, either autonomously or with human guidance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_rag_chain.invoke({\"question\": question})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
